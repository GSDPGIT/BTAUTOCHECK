#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BTAUTOCHECK æ•°æ®åˆ†æå¼•æ“
Analytics Engine for Trend Analysis and Statistics
"""

import json
import os
import glob
from datetime import datetime, timedelta
from collections import defaultdict
import statistics

class AnalyticsEngine:
    """æ•°æ®åˆ†æå¼•æ“"""
    
    def __init__(self):
        self.downloads_dir = 'downloads'
        self.config_file = 'config.json'
    
    def get_all_reports_data(self):
        """åŠ è½½æ‰€æœ‰å†å²æŠ¥å‘Šæ•°æ®"""
        reports = []
        pattern = os.path.join(self.downloads_dir, 'security_report_*.json')
        files = glob.glob(pattern)
        
        for filepath in files:
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # æå–å…³é”®ä¿¡æ¯
                    version = data.get('version', 'unknown')
                    check_time = data.get('check_time', '')
                    
                    static_analysis = data.get('static_analysis', {})
                    ai_analysis = data.get('ai_analysis', {})
                    
                    reports.append({
                        'version': version,
                        'check_time': check_time,
                        'static_score': static_analysis.get('security_score', 0),
                        'ai_score': ai_analysis.get('average_score', 0) if ai_analysis else 0,
                        'risk_files': static_analysis.get('risk_files_count', 0),
                        'total_issues': static_analysis.get('total_issues', 0),
                        'file_path': filepath
                    })
            except Exception as e:
                print(f"è¯»å–æŠ¥å‘Šå¤±è´¥ {filepath}: {e}")
                continue
        
        # æŒ‰æ—¶é—´æ’åº
        reports.sort(key=lambda x: x['check_time'])
        
        return reports
    
    def get_score_trend(self, days=30):
        """è·å–è¯„åˆ†è¶‹åŠ¿ï¼ˆæœ€è¿‘Nå¤©ï¼‰"""
        reports = self.get_all_reports_data()
        
        # è¿‡æ»¤æœ€è¿‘Nå¤©çš„æ•°æ®
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_reports = [
            r for r in reports 
            if self._parse_date(r['check_time']) >= cutoff_date
        ]
        
        if not recent_reports:
            return {
                'dates': [],
                'versions': [],
                'static_scores': [],
                'ai_scores': [],
                'avg_scores': []
            }
        
        # æ„å»ºè¶‹åŠ¿æ•°æ®
        trend = {
            'dates': [r['check_time'][:10] for r in recent_reports],  # YYYY-MM-DD
            'versions': [r['version'] for r in recent_reports],
            'static_scores': [r['static_score'] for r in recent_reports],
            'ai_scores': [r['ai_score'] for r in recent_reports],
            'avg_scores': [
                (r['static_score'] + r['ai_score']) / 2 if r['ai_score'] > 0 else r['static_score']
                for r in recent_reports
            ]
        }
        
        return trend
    
    def get_issue_distribution(self):
        """è·å–é—®é¢˜ç±»å‹åˆ†å¸ƒï¼ˆé¥¼å›¾æ•°æ®ï¼‰"""
        latest_report = self._get_latest_report()
        
        if not latest_report:
            return {}
        
        static_analysis = latest_report.get('static_analysis', {})
        deduction_details = static_analysis.get('deduction_details', {})
        
        # æ„å»ºåˆ†ç±»æ•°æ®
        distribution = {}
        
        category_map = {
            'backdoor_critical': 'é«˜å±åé—¨',
            'obfuscation_critical': 'ä»£ç æ··æ·†',
            'tracking_ads': 'è¿½è¸ªå¹¿å‘Š',
            'data_leak': 'æ•°æ®æ³„éœ²',
            'sql_injection_risk': 'SQLæ³¨å…¥',
            'suspicious_domain': 'å¯ç–‘åŸŸå',
            'privilege_escalation': 'æƒé™æå‡',
            'dangerous_functions': 'å±é™©å‡½æ•°',
            'command_execution': 'å‘½ä»¤æ‰§è¡Œ',
            'remote_connection': 'è¿œç¨‹è¿æ¥',
            'file_transfer': 'æ–‡ä»¶ä¼ è¾“'
        }
        
        for key, label in category_map.items():
            detail = deduction_details.get(key, {})
            count = detail.get('count', 0)
            if count > 0:
                distribution[label] = count
        
        return distribution
    
    def get_ai_usage_stats(self):
        """è·å–AIä½¿ç”¨ç»Ÿè®¡"""
        reports = self.get_all_reports_data()
        
        ai_stats = defaultdict(lambda: {'count': 0, 'total_score': 0, 'success': 0})
        
        for report in reports:
            filepath = report['file_path']
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    ai_analysis = data.get('ai_analysis', {})
                    
                    if ai_analysis:
                        provider = ai_analysis.get('provider', 'unknown')
                        score = ai_analysis.get('average_score', 0)
                        
                        ai_stats[provider]['count'] += 1
                        ai_stats[provider]['total_score'] += score
                        if score > 0:
                            ai_stats[provider]['success'] += 1
            except:
                continue
        
        # è®¡ç®—å¹³å‡åˆ†
        result = {}
        for provider, stats in ai_stats.items():
            result[provider] = {
                'count': stats['count'],
                'success_rate': (stats['success'] / stats['count'] * 100) if stats['count'] > 0 else 0,
                'avg_score': (stats['total_score'] / stats['count']) if stats['count'] > 0 else 0
            }
        
        return result
    
    def get_check_frequency_stats(self, days=30):
        """è·å–æ£€æµ‹é¢‘ç‡ç»Ÿè®¡"""
        reports = self.get_all_reports_data()
        
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_reports = [
            r for r in reports 
            if self._parse_date(r['check_time']) >= cutoff_date
        ]
        
        # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
        daily_counts = defaultdict(int)
        for r in recent_reports:
            date_key = r['check_time'][:10]
            daily_counts[date_key] += 1
        
        # å¡«å……ç©ºç™½æ—¥æœŸ
        all_dates = []
        current = cutoff_date.date()
        end = datetime.now().date()
        while current <= end:
            date_str = current.strftime('%Y-%m-%d')
            all_dates.append(date_str)
            if date_str not in daily_counts:
                daily_counts[date_str] = 0
            current += timedelta(days=1)
        
        return {
            'dates': all_dates,
            'counts': [daily_counts[d] for d in all_dates],
            'total_checks': len(recent_reports),
            'avg_per_day': len(recent_reports) / days if days > 0 else 0
        }
    
    def get_summary_stats(self):
        """è·å–æ±‡æ€»ç»Ÿè®¡"""
        reports = self.get_all_reports_data()
        
        if not reports:
            return {
                'total_reports': 0,
                'avg_static_score': 0,
                'avg_ai_score': 0,
                'highest_score': 0,
                'lowest_score': 0,
                'total_versions': 0
            }
        
        static_scores = [r['static_score'] for r in reports if r['static_score'] > 0]
        ai_scores = [r['ai_score'] for r in reports if r['ai_score'] > 0]
        all_scores = static_scores + ai_scores
        
        return {
            'total_reports': len(reports),
            'avg_static_score': round(statistics.mean(static_scores), 2) if static_scores else 0,
            'avg_ai_score': round(statistics.mean(ai_scores), 2) if ai_scores else 0,
            'highest_score': max(all_scores) if all_scores else 0,
            'lowest_score': min(all_scores) if all_scores else 0,
            'total_versions': len(set(r['version'] for r in reports)),
            'latest_check': reports[-1]['check_time'] if reports else 'Never'
        }
    
    def _get_latest_report(self):
        """è·å–æœ€æ–°æŠ¥å‘Š"""
        reports = self.get_all_reports_data()
        if not reports:
            return None
        
        latest = reports[-1]
        filepath = latest['file_path']
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return None
    
    def _parse_date(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        try:
            return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
        except:
            try:
                return datetime.strptime(date_str[:10], '%Y-%m-%d')
            except:
                return datetime.now()

# æµ‹è¯•
if __name__ == '__main__':
    engine = AnalyticsEngine()
    
    print("=" * 60)
    print("ğŸ“Š BTAUTOCHECK æ•°æ®åˆ†æå¼•æ“")
    print("=" * 60)
    
    # æ±‡æ€»ç»Ÿè®¡
    summary = engine.get_summary_stats()
    print(f"\nğŸ“ˆ æ±‡æ€»ç»Ÿè®¡:")
    print(f"  æ€»æŠ¥å‘Šæ•°: {summary['total_reports']}")
    print(f"  å¹³å‡é™æ€è¯„åˆ†: {summary['avg_static_score']}")
    print(f"  å¹³å‡AIè¯„åˆ†: {summary['avg_ai_score']}")
    print(f"  æœ€é«˜åˆ†: {summary['highest_score']}")
    print(f"  æœ€ä½åˆ†: {summary['lowest_score']}")
    
    # è¯„åˆ†è¶‹åŠ¿
    trend = engine.get_score_trend(30)
    print(f"\nğŸ“Š è¯„åˆ†è¶‹åŠ¿ï¼ˆæœ€è¿‘30å¤©ï¼‰:")
    print(f"  æ•°æ®ç‚¹: {len(trend['dates'])}")
    
    # é—®é¢˜åˆ†å¸ƒ
    distribution = engine.get_issue_distribution()
    print(f"\nğŸ” é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
    for category, count in distribution.items():
        print(f"  {category}: {count}")
    
    # AIä½¿ç”¨ç»Ÿè®¡
    ai_stats = engine.get_ai_usage_stats()
    print(f"\nğŸ¤– AIä½¿ç”¨ç»Ÿè®¡:")
    for provider, stats in ai_stats.items():
        print(f"  {provider}: {stats['count']}æ¬¡, æˆåŠŸç‡{stats['success_rate']:.1f}%, å¹³å‡åˆ†{stats['avg_score']:.1f}")
    
    print("\n" + "=" * 60)

